{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic patients readmission rates preditction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import optimize\n",
    "from sklearn import datasets as skdataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from learn2learn.algorithms.maml import MAML\n",
    "from learn2learn.data import TaskDataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project introduction\n",
    "\n",
    "- Overview: <br>\n",
    "This project is focusing on developing a predictive model to ascertain the likelihood of readmission for diabetes patients.\n",
    "<br>\n",
    "\n",
    "- Target:<br>\n",
    "The main goal of this project is developing a powerful machine learning model which can predict the readmission rate of patient "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The following cells are used to load training and testing data for our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pandas.read_csv(\"Dataset/diabetic_data_training.csv\")\n",
    "test_data = pandas.read_csv(\"Dataset/diabetic_data_test.csv\")\n",
    "mapping_info = pandas.read_csv(\"Dataset/IDS_mapping.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle different data types for plotting\n",
    "def plot_column(ax, column, df):\n",
    "    if df[column].dtype == 'object':\n",
    "        # Check if binary\n",
    "        if df[column].nunique() == 2:\n",
    "            # Binary data visualization\n",
    "            df[column].value_counts().plot(kind='bar', ax=ax)\n",
    "        else:\n",
    "            # Categorical data visualization\n",
    "            df[column].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=ax)\n",
    "    elif df[column].dtype == 'int64' or df[column].dtype == 'float64':\n",
    "        # Numeric data visualization\n",
    "        df[column].plot(kind='hist', bins=20, ax=ax)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"Unhandled data type for column: {column}\", \n",
    "                fontsize=12, ha='center')\n",
    "    ax.set_title(column)\n",
    "\n",
    "# Creating a 4x4 subplot layout\n",
    "fig, axes = plt.subplots(nrows=13, ncols=4, figsize=(20, 65))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "# Iterate through each column and plot\n",
    "for i, col in enumerate(train_data.columns):\n",
    "    # Adjust this line to select different subsets of columns  \n",
    "    plot_column(axes[i//4, i%4], col,train_data)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "The following cells are used to preprocess the training and testing data. There are two main goals in our preprocessing data section of the code\n",
    "- Change the string type data in our dataset to integer type data \n",
    "- Apply some applicable method to full up the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is used to change all string type data to integer type\n",
    "# for the missing value, we will skip and process it at next step\n",
    "df = train_data.copy()\n",
    "\n",
    "df.drop(columns = ['weight','encounter_id','patient_nbr', 'examide', 'citoglipton',\n",
    "'glimepiride-pioglitazone'],inplace=True)\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "df_test = test_data.copy()\n",
    "df_test.drop(columns = ['weight','encounter_id','patient_nbr', 'examide', 'citoglipton',\n",
    "'glimepiride-pioglitazone'],inplace=True)\n",
    "df_test.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = (df['age'].str.extract(r'(\\d+)-(\\d+)')[0].astype(int)+df['age'].str.extract(r'(\\d+)-(\\d+)')[1].astype(int))//2\n",
    "df_test['age'] = (df_test['age'].str.extract(r'(\\d+)-(\\d+)')[0].astype(int)+df_test['age'].str.extract(r'(\\d+)-(\\d+)')[1].astype(int))//2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['race'] != '?']\n",
    "print(df.any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding For race:\n",
    "cons: One-hot encoding can lead to a significant increase in the dataset's dimensionality (a problem known as the \"curse of dimensionality\"), especially if the categorical feature has many unique values. This can increase the computational cost and may require more data to achieve good performance.\n",
    "Dems Redct Would be apply, so it doesn't matter\n",
    "pros: Map to a fix number implies an ordinal relationship between the categories which may not exist, but is ideal for non-ordinal categorical data. It's suitable for many machine learning models, especially those that assume no ordinal relationship between categories\n",
    "\n",
    "\n",
    "1. random forest, remove ?\n",
    "2. randomly assign ? to a class by disstribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Missing value\n",
    "1. multiple imputation To be decide when training if less than 1h 5 epoch\n",
    "2. mean\n",
    "3. fullly remove\n",
    "4. wrong -> fix ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding for age:\n",
    "1. Asumming normal distribution, map to a random age in the range\n",
    "2. Map to mean age in the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(feature):\n",
    "    \"\"\"\n",
    "    Takes a series and one-hot encodes it.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.series): series containing a colum of the feature matrix.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A ndarray one-hot encoded.\n",
    "    \"\"\"\n",
    "    encoded_df = pd.get_dummies(feature).values\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['gender'] != 'Unknown/Invalid']\n",
    "Name = ['race','gender','change','diabetesMed']\n",
    "df_encoded = pd.get_dummies(df, columns=Name, prefix=Name)\n",
    "\n",
    "df_test_encoded = pd.get_dummies(df_test, columns=Name, prefix=Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {category: i for i, category in enumerate(df_encoded['medical_specialty'].unique())}\n",
    "df_encoded['medical_specialty'] = df_encoded['medical_specialty'].map(category_mapping)\n",
    "df_test_encoded['medical_specialty'] = df_test_encoded['medical_specialty'].map(category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {category: i for i, category in enumerate(df_encoded['payer_code'].unique())}\n",
    "df_encoded['payer_code'] = df_encoded['payer_code'].map(category_mapping)\n",
    "df_test_encoded['payer_code'] = df_test_encoded['payer_code'].map(category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medicion_mapping = {'No':0,'Down':1,'Steady':2,'Up':3}\n",
    "max_glu_serum_mapping = {'>200': 201, '>300': 301, 'normal': 0,}\n",
    "A1Cresult_mapping = {'>8':9,'>7':7.5,'normal':6}\n",
    "readmitted_mapping = {'NO':0,'<30':1,'>30':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['max_glu_serum'] = df_encoded['max_glu_serum'].map(max_glu_serum_mapping)\n",
    "\n",
    "df_test_encoded['max_glu_serum'] = df_test_encoded['max_glu_serum'].map(max_glu_serum_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded[\"A1Cresult\"] = df_encoded['A1Cresult'].map(A1Cresult_mapping)\n",
    "\n",
    "df_test_encoded[\"A1Cresult\"] = df_test_encoded['A1Cresult'].map(A1Cresult_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['readmitted'] = df_encoded['readmitted'].map(readmitted_mapping)\n",
    "\n",
    "df_test_encoded['readmitted'] = df_test_encoded['readmitted'].map(readmitted_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,name in enumerate(df_encoded.columns):\n",
    "    if(index >= 19 and index <= 38):\n",
    "        df_encoded[name] = df_encoded[name].map(medicion_mapping)\n",
    "\n",
    "for index,name in enumerate(df_test_encoded.columns):\n",
    "    if(index >= 19 and index <=38):\n",
    "        df_test_encoded[name] = df_test_encoded[name].map(medicion_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode payer_code, medical_specialty\n",
    "1. Find correlation internally with other feature in group of non-missing value\n",
    "2. Use identified feature predict payer-code, medical_specialty\n",
    "3. Prediction algorithm to be decide, could be KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode diag_1,diag_2,diag_3\n",
    "1. one hot\n",
    "2. ????????????? TBD\n",
    "3. remove missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoding all medicine:\n",
    "map to 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = ['admission_type_id','discharge_disposition_id','admission_source_id']\n",
    "for name in names:\n",
    "    category_means = df_encoded.groupby(name)['readmitted'].mean().reset_index()\n",
    "    category_means.columns = [name,name+'_readmitted_Mean']\n",
    "    df_encoded = pd.merge(df_encoded, category_means, on=name, how='left')\n",
    "    df_encoded = df_encoded.drop(name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['admission_type_id','discharge_disposition_id','admission_source_id']\n",
    "for name in names:\n",
    "    category_means = df_test_encoded.groupby(name)['readmitted'].mean().reset_index()\n",
    "    category_means.columns = [name,name+'_readmitted_Mean']\n",
    "    df_test_encoded = pd.merge(df_test_encoded, category_means, on=name, how='left')\n",
    "    df_test_encoded = df_test_encoded.drop(name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['diag_1'], inplace=True)\n",
    "df_encoded.drop(columns=['diag_2'], inplace=True)\n",
    "df_encoded.drop(columns=['diag_3'], inplace=True)\n",
    "df_encoded.drop(columns=['number_diagnoses'], inplace=True)\n",
    "\n",
    "df_test_encoded.drop(columns=['diag_1'], inplace=True)\n",
    "df_test_encoded.drop(columns=['diag_2'], inplace=True)\n",
    "df_test_encoded.drop(columns=['diag_3'], inplace=True)\n",
    "df_test_encoded.drop(columns=['number_diagnoses'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will recognise the ?, the missing value in training data\n",
    "# When we have recognised it, we will use **** method to full up it\n",
    "# the method we can discuss: \n",
    "#   delete\n",
    "#   mean,median or mode\n",
    "#   knn to predict\n",
    "#   Multiple Imputation\n",
    "predict_nan = df_encoded.copy()\n",
    "\n",
    "\n",
    "df_real_ms_index = predict_nan.index[predict_nan['medical_specialty'] != 0]\n",
    "unique_rows_index = predict_nan.index[predict_nan['medical_specialty'].duplicated(keep=False)]\n",
    "df_real_ms_index_total = df_real_ms_index.join(unique_rows_index,how = 'inner')\n",
    "\n",
    "df_real_pc_index = predict_nan.index[predict_nan['payer_code'] != 0]\n",
    "unique_rows_index = predict_nan.index[predict_nan['payer_code'].duplicated(keep=False)]\n",
    "df_real_pc_index_total = df_real_pc_index.join(unique_rows_index,how = 'inner')\n",
    "\n",
    "df_real_ms = predict_nan.loc[df_real_ms_index_total,['medical_specialty']]\n",
    "df_real_pc = predict_nan.loc[df_real_pc_index_total,['payer_code']]\n",
    "\n",
    "predict_nan.drop(columns = ['medical_specialty','payer_code'],inplace = True)\n",
    "\n",
    "df_data_train_ms = predict_nan.loc[df_real_ms_index_total]\n",
    "df_data_predict_ms = predict_nan.loc[~predict_nan.index.isin(df_real_ms_index)]\n",
    "\n",
    "df_data_train_pc = predict_nan.loc[df_real_pc_index_total]\n",
    "df_data_predict_pc = predict_nan.loc[~predict_nan.index.isin(df_real_pc_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will reduce the dimension our training data\n",
    "\n",
    "missing_value_predict_model = HistGradientBoostingClassifier(max_iter=100)\n",
    "missing_value_predict_model.fit(df_data_train_ms,df_real_ms['medical_specialty'])\n",
    "df_encoded['medical_specialty'].loc[~df_encoded['medical_specialty'].index.isin(df_real_ms_index)] = missing_value_predict_model.predict(df_data_predict_ms)\n",
    "\n",
    "\n",
    "\n",
    "missing_value_predict_model = HistGradientBoostingClassifier(max_iter=100)\n",
    "missing_value_predict_model.fit(df_data_train_pc,df_real_pc)\n",
    "df_encoded['payer_code'].loc[~df_encoded['payer_code'].index.isin(df_real_pc_index)] = missing_value_predict_model.predict(df_data_predict_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dems Redct\n",
    "1. PCA/PPCA\n",
    "2. LDA/QDA\n",
    "3. following to T-SNE\n",
    "3. Autoencoders\n",
    "4. Unsupervised Algorithmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Simulating Data\n",
    "np.random.seed(0)\n",
    "num_samples = 1000\n",
    "num_features = 5\n",
    "\n",
    "# Numerical data\n",
    "numeric_data = np.random.randn(num_samples, num_features)\n",
    "\n",
    "# Categorical data (let's say, colors)\n",
    "colors = ['Red', 'Green', 'Blue']\n",
    "categorical_data = np.random.choice(colors, size=num_samples)\n",
    "\n",
    "# Convert categorical data to one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_encoded = encoder.fit_transform(categorical_data.reshape(-1, 1))\n",
    "\n",
    "# Combining numerical and categorical data\n",
    "combined_data = np.hstack((numeric_data, categorical_encoded))\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of the variance\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "# Plotting the results\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "sns.scatterplot(x=tsne_result[:,0], y=tsne_result[:,1], hue=categorical_data, palette='bright')\n",
    "plt.title('t-SNE plot of the dataset')\n",
    "plt.xlabel('t-SNE Axis 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_shape = combined_data.shape[1]  # combined data from previous steps\n",
    "encoding_dim = 32  # example of encoding dimension\n",
    "\n",
    "# This is our input placeholder\n",
    "input_data = Input(shape=(input_shape,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_data)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_shape, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(combined_data, combined_data, epochs=50, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "We will build two models: \n",
    "1. A traditional machine learning model using Random Forest.\n",
    "2. A deep learning model using PyTorch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step we will try to use the Random Forest method to get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for Nerual Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "# Random Forest\n",
    "Y = df_encoded['readmitted']\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_encoded), columns=df_encoded.columns)\n",
    "X = df_normalized.drop('readmitted', axis=1)\n",
    "\n",
    "rf_classifier = HistGradientBoostingClassifier(max_iter=100, random_state=42)\n",
    "rf_classifier.fit(X, Y)\n",
    "\n",
    "# Nerual Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use testing dataset to predict\n",
    "Y_test = df_test_encoded['readmitted']\n",
    "df_test_normalized = pd.DataFrame(scaler.fit_transform(df_test_encoded), columns=df_test_encoded.columns)\n",
    "X_test = df_test_normalized.drop('readmitted', axis=1)\n",
    "\n",
    "Y_pred = rf_classifier.predict(X_test)\n",
    "print(Y_pred)\n",
    "print(Y_test)\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP0169",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
