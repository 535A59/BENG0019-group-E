{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetic patients readmission rates preditction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "# !pip install learn2learn\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import optimize\n",
    "from sklearn import datasets as skdataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from learn2learn.algorithms.maml import MAML\n",
    "from learn2learn.data import TaskDataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project introduction\n",
    "\n",
    "- Overview: <br>\n",
    "This project is focusing on developing a predictive model to ascertain the likelihood of readmission for diabetes patients.\n",
    "<br>\n",
    "\n",
    "- Target:<br>\n",
    "The main goal of this project is developing a powerful machine learning model which can predict the readmission rate of patient "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "The following cells are used to load training and testing data for our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Dataset/diabetic_data_training.csv\")\n",
    "test_data = pd.read_csv(\"Dataset/diabetic_data_test.csv\")\n",
    "mapping_info = pd.read_csv(\"Dataset/IDS_mapping.csv\", header=None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle different data types for plotting\n",
    "def plot_column(ax, column, df):\n",
    "    if df[column].dtype == 'object':\n",
    "        # Check if binary\n",
    "        if df[column].nunique() == 2:\n",
    "            # Binary data visualization\n",
    "            df[column].value_counts().plot(kind='bar', ax=ax)\n",
    "        else:\n",
    "            # Categorical data visualization\n",
    "            df[column].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=ax)\n",
    "    elif df[column].dtype == 'int64' or df[column].dtype == 'float64':\n",
    "        # Numeric data visualization\n",
    "        df[column].plot(kind='hist', bins=20, ax=ax)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f\"Unhandled data type for column: {column}\", \n",
    "                fontsize=12, ha='center')\n",
    "    ax.set_title(column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(nrows=13, ncols=4, figsize=(20, 65))\n",
    "# fig.tight_layout(pad=5.0)\n",
    "\n",
    "\n",
    "# for i, col in enumerate(train_data.columns):\n",
    "\n",
    "#     plot_column(axes[i//4, i%4], col,train_data)\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "The following cells are used to preprocess the training and testing data. There are two main goals in our preprocessing data section of the code\n",
    "- Change the string type data in our dataset to integer type data \n",
    "- Apply some applicable method to full up the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is used to change all string type data to integer type\n",
    "# for the missing value, we will skip and process it at next step\n",
    "\n",
    "# df = train_data.copy()\n",
    "\n",
    "# df.drop(columns = ['weight','encounter_id','patient_nbr', 'examide', 'citoglipton',\n",
    "# 'glimepiride-pioglitazone'],inplace=True)\n",
    "# df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# df_test = test_data.copy()\n",
    "# df_test.drop(columns = ['weight','encounter_id','patient_nbr', 'examide', 'citoglipton',\n",
    "# 'glimepiride-pioglitazone'],inplace=True)\n",
    "# df_test.replace('?', np.nan, inplace=True)\n",
    "df = train_data.copy()\n",
    "df_test = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding For race:\n",
    "cons: One-hot encoding can lead to a significant increase in the dataset's dimensionality (a problem known as the \"curse of dimensionality\"), especially if the categorical feature has many unique values. This can increase the computational cost and may require more data to achieve good performance.\n",
    "Dems Redct Would be apply, so it doesn't matter\n",
    "pros: Map to a fix number implies an ordinal relationship between the categories which may not exist, but is ideal for non-ordinal categorical data. It's suitable for many machine learning models, especially those that assume no ordinal relationship between categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Race Process\n",
    "1. Remove missing since race is proved to be a significant impact to medical result.\n",
    "2. One-hot encode race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_filter(df):\n",
    "    \"\"\"\n",
    "    Remove missing value from race\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.dataframe): series containing a colum of the feature matrix.\n",
    "\n",
    "    Return:\n",
    "    Filtered dataframe free from ? for race\n",
    "    \"\"\"\n",
    "    race_mask = (df['race'] != \"?\")\n",
    "    df = df[race_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = race_filter(df)\n",
    "df_encoded = pd.get_dummies(df, columns=[\"race\"], prefix=\"race\",dtype=int)\n",
    "\n",
    "df_test = race_filter(df_test)\n",
    "df_test_encoded = pd.get_dummies(df_test, columns=[\"race\"], prefix=\"race\",dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Process\n",
    "\n",
    "1. Remove Unknown/Invalid and missing\n",
    "2. One hot encode Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_filter(df):\n",
    "    \"\"\"\n",
    "    Remove missing value from race\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.dataframe): series containing a colum of the feature matrix.\n",
    "\n",
    "    Return:\n",
    "    Filtered dataframe free from ? for race\n",
    "    \"\"\"\n",
    "    gender_mask = (df['gender'] != \"Unknown/Invalid\")\n",
    "    df = df[gender_mask]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = gender_filter(df_encoded)\n",
    "df_test_encoded = gender_filter(df_test_encoded)\n",
    "gender_mapping = {'Male':0,'Female':1}\n",
    "df_encoded['gender'] = df_encoded['gender'].map(gender_mapping)\n",
    "df_test_encoded['gender'] = df_test_encoded['gender'].map(gender_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encounter_id and patient_nbr Process \n",
    "1. n/a\n",
    "2. Drop since it provide no info to the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['encounter_id','patient_nbr'], inplace=True)\n",
    "df_test_encoded.drop(columns=['encounter_id','patient_nbr'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Process\n",
    "1. n/a\n",
    "2. Map age from range to mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['age'] = (df['age'].str.extract(r'(\\d+)-(\\d+)')[0].astype(int)+df['age'].str.extract(r'(\\d+)-(\\d+)')[1].astype(int))//2\n",
    "df_test_encoded['age'] = (df_test['age'].str.extract(r'(\\d+)-(\\d+)')[0].astype(int)+df_test['age'].str.extract(r'(\\d+)-(\\d+)')[1].astype(int))//2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Process\n",
    "1. Drop Weight for too many missing values and no information to predict.\n",
    "2. n/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['weight'], inplace=True)\n",
    "df_test_encoded.drop(columns=['weight'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_lab_procedures num_procedures num_medications number_outpatient number_emergency number_inpatient No need to be encode since no missing and integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diag_1,diag_2 and diag_3 Process:\n",
    "\n",
    "Filte diag_1, diag_2 and diag_3 by number_diagnoses. If the amount of diagnoses in diag_1, diag_2 and diag_3 doesn't match the number_diagnoses, then remove.\n",
    "Layer encoding diags according to the ICD-9 Code Category, drop original 3 diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_missing_value_filter(df):\n",
    "\n",
    "    df['number_diagnoses'] = pd.to_numeric(df['number_diagnoses'], errors='coerce')\n",
    "    mask = ((df['number_diagnoses'] <= 3) & (\n",
    "            (3 - df[['diag_1', 'diag_2', 'diag_3']].apply(lambda x: x == \"?\").sum(axis=1)) >= df['number_diagnoses'])) | (\n",
    "    (df['number_diagnoses'] > 3) & (df[['diag_1', 'diag_2', 'diag_3']].apply(lambda x: x == \"?\").sum(axis=1)) == 0) \n",
    "    df_filtered = df[mask].copy()\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def diag_layer_encoding(df):\n",
    "\n",
    "    # Define the encoding function\n",
    "    def encode_element(element):\n",
    "        encoding_result = [0] * 20  # Default encoding\n",
    "\n",
    "        if pd.notna(element):\n",
    "            if element[0].isdigit():\n",
    "                element = float(element)\n",
    "                # Encode based on numeric range\n",
    "                if 1 <= element <= 139:\n",
    "                    encoding_result[0] = 1\n",
    "                elif 140 <= element <= 239:\n",
    "                    encoding_result[1] = 1\n",
    "                elif 240 <= element <= 279:\n",
    "                    encoding_result[2] = 1\n",
    "                elif 280 <= element <= 289:\n",
    "                    encoding_result[3] = 1\n",
    "                elif 290 <= element <= 319:\n",
    "                    encoding_result[4] = 1\n",
    "                elif 320 <= element <= 389:\n",
    "                    encoding_result[5] = 1\n",
    "                elif 390 <= element <= 459:\n",
    "                    encoding_result[6] = 1\n",
    "                elif 460 <= element <= 519:\n",
    "                    encoding_result[7] = 1\n",
    "                elif 520 <= element <= 579:\n",
    "                    encoding_result[8] = 1\n",
    "                elif 580 <= element <= 629:\n",
    "                    encoding_result[9] = 1\n",
    "                elif 630 <= element <= 679:\n",
    "                    encoding_result[10] = 1\n",
    "                elif 680 <= element <= 709:\n",
    "                    encoding_result[11] = 1\n",
    "                elif 710 <= element <= 739:\n",
    "                    encoding_result[12] = 1\n",
    "                elif 740 <= element <= 759:\n",
    "                    encoding_result[13] = 1\n",
    "                elif 760 <= element <= 779:\n",
    "                    encoding_result[14] = 1\n",
    "                elif 780 <= element <= 799:\n",
    "                    encoding_result[15] = 1\n",
    "                elif 800 <= element <= 999:\n",
    "                    encoding_result[16] = 1\n",
    "                # Add more conditions for other ranges if needed\n",
    "            elif element[0].isalpha():\n",
    "                # Encode based on string prefix\n",
    "                if element.startswith('E'):\n",
    "                    encoding_result[17] = 1\n",
    "                elif element.startswith('V'):\n",
    "                    encoding_result[18] = 1\n",
    "                elif element.startswith('M'):\n",
    "                    encoding_result[19] = 1\n",
    "                # Add more conditions for other prefixes if needed\n",
    "\n",
    "        return encoding_result\n",
    "\n",
    "    for i in range(1,4):\n",
    "        encoded_columns = df[\"diag_\"+str(i)].apply(encode_element)\n",
    "\n",
    "    # Create new columns with \"diag_1\" as a prefix\n",
    "        for j in range(20):\n",
    "            new_column_name = \"diag_\" + str(i)+\"_\"+ str(j+1)\n",
    "            df[new_column_name] = encoded_columns.apply(lambda x: x[j])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = diag_missing_value_filter(df_encoded)\n",
    "df_encoded = diag_layer_encoding(df_encoded)\n",
    "df_test_encoded = diag_missing_value_filter(df_test_encoded)\n",
    "df_test_encoded = diag_layer_encoding(df_test_encoded)\n",
    "\n",
    "for i in range(1,4):\n",
    "        df_encoded.drop(columns=[f'diag_{i}'], inplace=True)\n",
    "        df_test_encoded.drop(columns=[f'diag_{i}'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number_diagnoses pass since no missing and integer max_glu_serum and A1Cresult Process: \n",
    "1. n/a \n",
    "2. index map to 0-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_glu_serum_mapping = {'>200': 2, '>300': 3, 'normal': 1}\n",
    "A1Cresult_mapping = {'>8':3,'>7':2,'normal':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['max_glu_serum'] = df_encoded['max_glu_serum'].map(max_glu_serum_mapping).fillna(0)\n",
    "df_test_encoded['max_glu_serum'] = df_test_encoded['max_glu_serum'].map(max_glu_serum_mapping).fillna(0)\n",
    "\n",
    "df_encoded[\"A1Cresult\"] = df_encoded['A1Cresult'].map(A1Cresult_mapping).fillna(0)\n",
    "df_test_encoded[\"A1Cresult\"] = df_test_encoded['A1Cresult'].map(A1Cresult_mapping).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metformin repaglinide nateglinide chlorpropamide glimepiride acetohexamide glipizide glyburide tolbutamide pioglitazone rosiglitazone acarbose miglitol troglitazone tolazamide examide citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone Process:\n",
    "\n",
    "1. drop 'examide', 'citoglipton','glimepiride-pioglitazone' and 'metformin-rosiglitazone' since single value observed\n",
    "2. index map to 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.drop(columns=['examide', 'citoglipton','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1,inplace=True)\n",
    "df_test_encoded.drop(columns=['examide', 'citoglipton','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1,inplace=True)\n",
    "medics = ['metformin','repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "       'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "       'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "        'metformin-pioglitazone']\n",
    "for i in medics:\n",
    "    df_encoded.loc[df_encoded[i] == 'Up', [i]] = 3  \n",
    "    df_encoded.loc[df_encoded[i] == 'Down', [i]] = 1 \n",
    "    df_encoded.loc[df_encoded[i] == 'Steady', [i]] = 2 \n",
    "    df_encoded.loc[df_encoded[i] == 'No', [i]] = 0\n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Up', [i]] = 3  \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Down', [i]] = 1 \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'Steady', [i]] = 2 \n",
    "    df_test_encoded.loc[df_test_encoded[i] == 'No', [i]] = 0 \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change and diabetesMed Process\n",
    "\n",
    "1. n/a\n",
    "2. binary map to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_mapping = {'No':0,'Ch':1}\n",
    "diabetesMed_mapping = {'No':0,'Yes':1}\n",
    "\n",
    "df_encoded['change'] = df_encoded['change'].map(change_mapping)\n",
    "df_encoded['diabetesMed'] = df_encoded['diabetesMed'].map(diabetesMed_mapping)\n",
    "\n",
    "df_test_encoded['change'] = df_test_encoded['change'].map(change_mapping)\n",
    "df_test_encoded['diabetesMed'] = df_test_encoded['diabetesMed'].map(diabetesMed_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readmitted Process: \n",
    "1. n/a \n",
    "2. index mapping, No as 0, >30 as 1 and <30 as most significant as 2 and drop readmitted for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "readmitted_mapping = {'NO':0,'<30':2,'>30':1}\n",
    "df_encoded['readmitted'] = df_encoded['readmitted'].map(readmitted_mapping)\n",
    "df_test_encoded['readmitted'] = df_test_encoded['readmitted'].map(readmitted_mapping)\n",
    "\n",
    "y_test_readmitted = df_encoded[\"readmitted\"]\n",
    "y_readmitted = df_test_encoded[\"readmitted\"]\n",
    "\n",
    "df_encoded.drop(columns=[\"readmitted\"],axis=1,inplace=True)\n",
    "df_test_encoded.drop(columns=[\"readmitted\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'admission_type_id','discharge_disposition_id','admission_source_id' Process:\n",
    "1. n/a\n",
    "2. target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_encoded\n",
    "Y_train = y_test_readmitted\n",
    "\n",
    "features = ['admission_type_id','discharge_disposition_id','admission_source_id']\n",
    "\n",
    "'''for name in names:\n",
    "    category_means = df_encoded.groupby(name)['readmitted'].mean().reset_index()\n",
    "    category_means.columns = [name,name+'_readmitted_Mean']\n",
    "    df_encoded = pd.merge(df_encoded, category_means, on=name, how='left')\n",
    "    df_encoded = df_encoded.drop(name, axis=1)'''\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "    \n",
    "    mean_col_name = f'{feature}_Encoded'\n",
    "    means = X_train.join(Y_train).groupby(feature)['readmitted'].mean()\n",
    "    df_encoded[mean_col_name] = df_encoded[feature].map(means)\n",
    "    df_test_encoded[mean_col_name] = df_test_encoded[feature].map(means)\n",
    "\n",
    "    df_encoded = df_encoded.drop(feature, axis=1)\n",
    "    df_test_encoded = df_test_encoded.drop(feature, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payer Code Process:\n",
    "\n",
    "1.2 RandomForestClassifier fill missing\n",
    "1.3 KNN filling missing\n",
    "\n",
    "2. index encoding payer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payer_code_index_encoding(df):\n",
    "    df.loc[df['payer_code'] == 'MD', ['payer_code']] = 1\n",
    "    df.loc[df['payer_code'] == 'MC', ['payer_code']] = 2\n",
    "    df.loc[df['payer_code'] == 'HM', ['payer_code']] = 3\n",
    "    df.loc[df['payer_code'] == 'UN', ['payer_code']] = 4\n",
    "    df.loc[df['payer_code'] == 'BC', ['payer_code']] = 5\n",
    "    df.loc[df['payer_code'] == 'CP', ['payer_code']] = 6\n",
    "    df.loc[df['payer_code'] == 'SP', ['payer_code']] = 7\n",
    "    df.loc[df['payer_code'] == 'SI', ['payer_code']] = 8\n",
    "    df.loc[df['payer_code'] == 'CM', ['payer_code']] = 9\n",
    "    df.loc[df['payer_code'] == 'DM', ['payer_code']] = 10\n",
    "    df.loc[df['payer_code'] == 'CH', ['payer_code']] = 11\n",
    "    df.loc[df['payer_code'] == 'PO', ['payer_code']] = 12\n",
    "    df.loc[df['payer_code'] == 'WC', ['payer_code']] = 13\n",
    "    df.loc[df['payer_code'] == 'OG', ['payer_code']] = 14\n",
    "    df.loc[df['payer_code'] == 'OT', ['payer_code']] = 15\n",
    "    df.loc[df['payer_code'] == 'MP', ['payer_code']] = 16\n",
    "    df.loc[df['payer_code'] == 'FR', ['payer_code']] = 17\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = payer_code_index_encoding(df_encoded)\n",
    "df_test_encoded = payer_code_index_encoding(df_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the columns that contain null values and need to predicted.\n",
    "payer_code_col = ['payer_code']\n",
    "other_cols = list(df_encoded.columns)\n",
    "other_cols.remove(\"payer_code\")\n",
    "other_cols.remove(\"medical_specialty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "payer_code_mask = (df_encoded[\"payer_code\"] != \"?\")\n",
    "df_notnans = df_encoded[payer_code_mask]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_notnans[other_cols], df_notnans[payer_code_col].astype(int),\n",
    "                                                    train_size=0.75,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "# paramsgrid  = {'n_estimators': [100,200,400],\n",
    "#                'max_depth': [25, 30,50,100],\n",
    "#  'min_samples_leaf': [1, 2, 3],\n",
    "#  'min_samples_split': [2, 5, 10]\n",
    "#  }\n",
    "\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = paramsgrid, n_iter = 5, cv = 3, verbose=2, random_state=1, n_jobs = -1)\n",
    " \n",
    "# rf_random.fit(X_test, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.5625093241832015\n"
     ]
    }
   ],
   "source": [
    "rf1 =(RandomForestClassifier(max_depth=100,n_estimators = 400,min_samples_leaf=1,min_samples_split=10))\n",
    "rf1.fit(X_train, y_train.values.ravel())\n",
    "score = rf1.score(X_test, y_test.values.ravel())\n",
    "\n",
    "print(f\"Score is {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_score = []\n",
    "# test_score = []\n",
    "\n",
    "# for i in range(6,30,2):\n",
    "\n",
    "#     knn = KNeighborsClassifier(i)\n",
    "#     knn.fit(X_train,y_train.values.ravel())\n",
    "    \n",
    "#     train_score.append(knn.score(X_train,y_train.values.ravel()))\n",
    "#     test_score.append(knn.score(X_test,y_test.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_score)\n",
    "# plt.plot(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn2 =(KNeighborsClassifier(n_neighbors= 21))\n",
    "# # # Fit on the train data\n",
    "# knn2.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# # # Check the prediction score\n",
    "# score = knn2.score(X_test, y_test.values.ravel())\n",
    "# print(f\"The score on the testset is {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "56>53, therefore use randomforst for medical_specialty and payercode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104779/2230776403.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_notnans[\"payer_code\"] = rf1.predict(df_notnans[other_cols])\n"
     ]
    }
   ],
   "source": [
    "payer_code_mask = (df_encoded[\"payer_code\"] == \"?\")\n",
    "df_notnans = df_encoded[payer_code_mask]\n",
    "df_notnans[\"payer_code\"] = rf1.predict(df_notnans[other_cols])\n",
    "df_encoded.loc[payer_code_mask, 'payer_code'] = df_notnans['payer_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104779/1322505668.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_notnans[\"payer_code\"] = rf1.predict(df_notnans[other_cols])\n"
     ]
    }
   ],
   "source": [
    "test_payer_code_mask = (df_test_encoded[\"payer_code\"] == \"?\")\n",
    "df_notnans = df_test_encoded[test_payer_code_mask]\n",
    "df_notnans[\"payer_code\"] = rf1.predict(df_notnans[other_cols])\n",
    "df_test_encoded.loc[test_payer_code_mask, 'payer_code'] = df_notnans['payer_code']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "medical_specialty Proces:\n",
    "1. index encoding label\n",
    "2. RF and KNN filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_medical_specialty = df_encoded[\"medical_specialty\"].unique()\n",
    "test_medical_specialty = df_test_encoded[\"medical_specialty\"].unique()\n",
    "\n",
    "train_medical_specialty = list(train_medical_specialty)\n",
    "train_medical_specialty.remove(\"?\")\n",
    "train_medical_specialty = set(train_medical_specialty)\n",
    "\n",
    "test_medical_specialty = list(test_medical_specialty)\n",
    "test_medical_specialty.remove(\"?\")\n",
    "test_medical_specialty = set(test_medical_specialty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104779/2194747546.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  medical_specialty_test_to_be_encoded['medical_specialty']= medical_specialty_test_to_be_encoded['medical_specialty'].map(lambda x: medical_specialty_dict[x])\n",
      "/tmp/ipykernel_104779/2194747546.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  medical_specialty_to_be_encoded['medical_specialty'] = medical_specialty_to_be_encoded['medical_specialty'].map(lambda x: medical_specialty_dict[x])\n"
     ]
    }
   ],
   "source": [
    "medical_specialty_lable = train_medical_specialty.union(test_medical_specialty)\n",
    "\n",
    "medical_specialty_nan_mask = (df_encoded['medical_specialty'] != \"?\")\n",
    "medical_specialty_test_nan_mask = (df_test_encoded['medical_specialty'] != \"?\")\n",
    "\n",
    "medical_specialty_dict = {element: index for index, element in enumerate(medical_specialty_lable)}\n",
    "\n",
    "\n",
    "medical_specialty_test_to_be_encoded = df_test_encoded[medical_specialty_test_nan_mask]\n",
    "medical_specialty_test_to_be_encoded['medical_specialty']= medical_specialty_test_to_be_encoded['medical_specialty'].map(lambda x: medical_specialty_dict[x])\n",
    "\n",
    "medical_specialty_to_be_encoded = df_encoded[medical_specialty_nan_mask]\n",
    "medical_specialty_to_be_encoded['medical_specialty'] = medical_specialty_to_be_encoded['medical_specialty'].map(lambda x: medical_specialty_dict[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['medical_specialty'] = medical_specialty_to_be_encoded['medical_specialty']\n",
    "df_test_encoded['medical_specialty'] = medical_specialty_test_to_be_encoded['medical_specialty']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF for medical_specialty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=100, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=100, min_samples_leaf=3, min_samples_split=2, n_estimators=200; total time=   3.0s\n",
      "[CV] END max_depth=100, min_samples_leaf=3, min_samples_split=2, n_estimators=200; total time=   2.9s\n",
      "[CV] END max_depth=100, min_samples_leaf=3, min_samples_split=2, n_estimators=200; total time=   3.1s\n",
      "[CV] END max_depth=100, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=100, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=100, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   6.7s\n",
      "[CV] END max_depth=100, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   6.6s\n",
      "[CV] END max_depth=100, min_samples_leaf=2, min_samples_split=2, n_estimators=400; total time=   6.8s\n",
      "[CV] END max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   6.7s\n",
      "[CV] END max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   6.4s\n",
      "[CV] END max_depth=100, min_samples_leaf=1, min_samples_split=10, n_estimators=400; total time=   6.3s\n",
      "[CV] END max_depth=100, min_samples_leaf=3, min_samples_split=2, n_estimators=400; total time=   5.7s\n",
      "[CV] END max_depth=100, min_samples_leaf=3, min_samples_split=2, n_estimators=400; total time=   5.5s\n",
      "[CV] END max_depth=100, min_samples_leaf=3, min_samples_split=2, n_estimators=400; total time=   3.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=5,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [25, 30, 50, 100],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 3],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 400]},\n",
       "                   random_state=1, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=5,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [25, 30, 50, 100],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 3],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 400]},\n",
       "                   random_state=1, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=5,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [25, 30, 50, 100],\n",
       "                                        'min_samples_leaf': [1, 2, 3],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 400]},\n",
       "                   random_state=1, verbose=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_specialty_mask = df_encoded[\"medical_specialty\"].notnull()\n",
    "df_notnans = df_encoded[medical_specialty_mask]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_notnans[other_cols], df_notnans[\"medical_specialty\"].astype(int),\n",
    "                                                    train_size=0.75,\n",
    "                                                    random_state=1)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "paramsgrid  = {'n_estimators': [100,200,400],\n",
    "               'max_depth': [25, 30,50,100],\n",
    " 'min_samples_leaf': [1, 2, 3],\n",
    " 'min_samples_split': [2, 5, 10]\n",
    " }\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = paramsgrid, n_iter = 5, cv = 3, verbose=2, random_state=1, n_jobs = -1)\n",
    " \n",
    "rf_random.fit(X_test, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_depth': 100}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.5383195640710142\n"
     ]
    }
   ],
   "source": [
    "rf1 =(RandomForestClassifier(max_depth=100,n_estimators = 400,min_samples_leaf=2,min_samples_split=2))\n",
    "rf1.fit(X_train, y_train.values.ravel())\n",
    "score = rf1.score(X_test, y_test.values.ravel())\n",
    "\n",
    "print(f\"Score is {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN for medical_specialty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_score = []\n",
    "# test_score = []\n",
    "\n",
    "# for i in range(5,100,2):\n",
    "\n",
    "#     knn = KNeighborsClassifier(i)\n",
    "#     knn.fit(X_train,y_train.values.ravel())\n",
    "#     trianscore = knn.score(X_train,y_train.values.ravel())\n",
    "#     testscore = knn.score(X_test,y_test.values.ravel())\n",
    "#     train_score.append(trianscore)\n",
    "#     test_score.append(testscore)\n",
    "#     print(f\"train score is: {trianscore}, test score is: {testscore}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_score)\n",
    "# plt.plot(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn2 =(KNeighborsClassifier(n_neighbors= 25))\n",
    "# # # Fit on the train data\n",
    "# knn2.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# # # Check the prediction score\n",
    "# score = knn2.score(X_test, y_test.values.ravel())\n",
    "# print(f\"The score on the testset is {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "53>35, therefore use RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104779/2029475354.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_notnans[\"medical_specialty\"] = rf1.predict(df_notnans[other_cols])\n"
     ]
    }
   ],
   "source": [
    "medical_specialty_code_mask = df_encoded[\"medical_specialty\"].isna()\n",
    "df_notnans = df_encoded[medical_specialty_code_mask]\n",
    "df_notnans[\"medical_specialty\"] = rf1.predict(df_notnans[other_cols])\n",
    "df_encoded.loc[medical_specialty_code_mask, 'medical_specialty'] = df_notnans['medical_specialty']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part will recognise the ?, the missing value in training data\n",
    "# When we have recognised it, we will use **** method to full up it\n",
    "# the method we can discuss: \n",
    "#   delete\n",
    "#   mean,median or mode\n",
    "#   knn to predict\n",
    "#   Multiple Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict_nan = df_encoded.copy()\n",
    "\n",
    "\n",
    "# df_real_ms_index = predict_nan.index[predict_nan['medical_specialty'] != 0]\n",
    "# unique_rows_index = predict_nan.index[predict_nan['medical_specialty'].duplicated(keep=False)]\n",
    "# df_real_ms_index_total = df_real_ms_index.join(unique_rows_index,how = 'inner')\n",
    "\n",
    "# df_real_pc_index = predict_nan.index[predict_nan['payer_code'] != 0]\n",
    "# unique_rows_index = predict_nan.index[predict_nan['payer_code'].duplicated(keep=False)]\n",
    "# df_real_pc_index_total = df_real_pc_index.join(unique_rows_index,how = 'inner')\n",
    "\n",
    "# df_real_ms = predict_nan.loc[df_real_ms_index_total,['medical_specialty']]\n",
    "# df_real_pc = predict_nan.loc[df_real_pc_index_total,['payer_code']]\n",
    "\n",
    "# predict_nan.drop(columns = ['medical_specialty','payer_code'],inplace = True)\n",
    "\n",
    "# df_data_train_ms = predict_nan.loc[df_real_ms_index_total]\n",
    "# df_data_predict_ms = predict_nan.loc[~predict_nan.index.isin(df_real_ms_index)]\n",
    "\n",
    "# df_data_train_pc = predict_nan.loc[df_real_pc_index_total]\n",
    "# df_data_predict_pc = predict_nan.loc[~predict_nan.index.isin(df_real_pc_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this part will reduce the dimension our training data\n",
    "\n",
    "# missing_value_predict_model = HistGradientBoostingClassifier(max_iter=100)\n",
    "# missing_value_predict_model.fit(df_data_train_ms,df_real_ms['medical_specialty'])\n",
    "# df_encoded['medical_specialty'].loc[~df_encoded['medical_specialty'].index.isin(df_real_ms_index)] = missing_value_predict_model.predict(df_data_predict_ms)\n",
    "\n",
    "# missing_value_predict_model = HistGradientBoostingClassifier(max_iter=100)\n",
    "# missing_value_predict_model.fit(df_data_train_pc,df_real_pc)\n",
    "# df_encoded['payer_code'].loc[~df_encoded['payer_code'].index.isin(df_real_pc_index)] = missing_value_predict_model.predict(df_data_predict_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dems Redct\n",
    "1. PCA/PPCA\n",
    "2. LDA/QDA\n",
    "3. following to T-SNE\n",
    "3. Autoencoders\n",
    "4. Unsupervised Algorithmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szy/Downloads/yes/envs/COMP0169/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[198], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m tsne_result \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit_transform(pca_result)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Plotting the results\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mset(rc\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.figsize\u001b[39m\u001b[38;5;124m'\u001b[39m:(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m8\u001b[39m)})\n\u001b[1;32m     40\u001b[0m sns\u001b[38;5;241m.\u001b[39mscatterplot(x\u001b[38;5;241m=\u001b[39mtsne_result[:,\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mtsne_result[:,\u001b[38;5;241m1\u001b[39m], hue\u001b[38;5;241m=\u001b[39mcategorical_data, palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbright\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt-SNE plot of the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Simulating Data\n",
    "np.random.seed(0)\n",
    "num_samples = 1000\n",
    "num_features = 5\n",
    "\n",
    "# Numerical data\n",
    "numeric_data = np.random.randn(num_samples, num_features)\n",
    "\n",
    "# Categorical data (let's say, colors)\n",
    "colors = ['Red', 'Green', 'Blue']\n",
    "categorical_data = np.random.choice(colors, size=num_samples)\n",
    "\n",
    "# Convert categorical data to one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_encoded = encoder.fit_transform(categorical_data.reshape(-1, 1))\n",
    "\n",
    "# Combining numerical and categorical data\n",
    "combined_data = np.hstack((numeric_data, categorical_encoded))\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of the variance\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "tsne_result = tsne.fit_transform(pca_result)\n",
    "\n",
    "# Plotting the results\n",
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "sns.scatterplot(x=tsne_result[:,0], y=tsne_result[:,1], hue=categorical_data, palette='bright')\n",
    "plt.title('t-SNE plot of the dataset')\n",
    "plt.xlabel('t-SNE Axis 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_shape = combined_data.shape[1]  # combined data from previous steps\n",
    "encoding_dim = 32  # example of encoding dimension\n",
    "\n",
    "# This is our input placeholder\n",
    "input_data = Input(shape=(input_shape,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_data)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_shape, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(combined_data, combined_data, epochs=50, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "We will build two models: \n",
    "1. A traditional machine learning model using Random Forest.\n",
    "2. A deep learning model using PyTorch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step we will try to use the Random Forest method to get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for Nerual Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1Cresult\n",
       "0.0    76279\n",
       "3.0     7398\n",
       "1.0     4479\n",
       "2.0     3430\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded['A1Cresult'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                                         True\n",
       "time_in_hospital                            True\n",
       "payer_code                                  True\n",
       "medical_specialty                           True\n",
       "num_lab_procedures                          True\n",
       "num_procedures                              True\n",
       "num_medications                             True\n",
       "number_outpatient                           True\n",
       "number_emergency                            True\n",
       "number_inpatient                            True\n",
       "max_glu_serum                               True\n",
       "A1Cresult                                   True\n",
       "metformin                                   True\n",
       "repaglinide                                 True\n",
       "nateglinide                                 True\n",
       "chlorpropamide                              True\n",
       "glimepiride                                 True\n",
       "acetohexamide                               True\n",
       "glipizide                                   True\n",
       "glyburide                                   True\n",
       "tolbutamide                                 True\n",
       "pioglitazone                                True\n",
       "rosiglitazone                               True\n",
       "acarbose                                    True\n",
       "miglitol                                    True\n",
       "troglitazone                                True\n",
       "tolazamide                                  True\n",
       "insulin                                     True\n",
       "glyburide-metformin                         True\n",
       "glipizide-metformin                         True\n",
       "metformin-rosiglitazone                     True\n",
       "metformin-pioglitazone                      True\n",
       "readmitted                                  True\n",
       "race_AfricanAmerican                        True\n",
       "race_Asian                                  True\n",
       "race_Caucasian                              True\n",
       "race_Hispanic                               True\n",
       "race_Other                                  True\n",
       "gender_Female                               True\n",
       "gender_Male                                 True\n",
       "change_Ch                                   True\n",
       "change_No                                   True\n",
       "diabetesMed_No                              True\n",
       "diabetesMed_Yes                             True\n",
       "admission_type_id_readmitted_Mean           True\n",
       "discharge_disposition_id_readmitted_Mean    True\n",
       "admission_source_id_readmitted_Mean         True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_normalized.astype(float) < 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HistGradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training loop\n",
    "# Random Forest\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# 选择最好的K个特征\n",
    "\n",
    "\n",
    "Y = df_encoded['readmitted']\n",
    "#df_normalized = pd.DataFrame(scaler.fit_transform(df_encoded), columns=df_encoded.columns)\n",
    "X = df_encoded.drop('readmitted', axis=1)\n",
    "\n",
    "selector = SelectKBest(chi2, k=10)\n",
    "X_new = selector.fit_transform(X, Y)\n",
    "\n",
    "\n",
    "rf_classifier = HistGradientBoostingClassifier(max_iter=100, random_state=42)\n",
    "rf_classifier.fit(X, Y)\n",
    "\n",
    "# Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100)\n",
    "classifier.fit(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 2]\n",
      "0        0\n",
      "1        2\n",
      "2        0\n",
      "3        0\n",
      "4        2\n",
      "        ..\n",
      "10172    2\n",
      "10173    0\n",
      "10174    0\n",
      "10175    1\n",
      "10176    2\n",
      "Name: readmitted, Length: 10177, dtype: int64\n",
      "0.5851429694408962\n"
     ]
    }
   ],
   "source": [
    "# use testing dataset to predict\n",
    "Y_test = df_test_encoded['readmitted']\n",
    "#df_test_normalized = pd.DataFrame(scaler.fit_transform(df_test_encoded), columns=df_test_encoded.columns)\n",
    "X_test = df_test_encoded.drop('readmitted', axis=1)\n",
    "#X_test = selector.transform(X_test)\n",
    "\n",
    "Y_pred = rf_classifier.predict(X_test)\n",
    "print(Y_pred)\n",
    "print(Y_test)\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 2]\n",
      "0        0\n",
      "1        2\n",
      "2        0\n",
      "3        0\n",
      "4        2\n",
      "        ..\n",
      "10172    2\n",
      "10173    0\n",
      "10174    0\n",
      "10175    1\n",
      "10176    2\n",
      "Name: readmitted, Length: 10177, dtype: int64\n",
      "0.5586125577282107\n"
     ]
    }
   ],
   "source": [
    "Y_pred = classifier.predict(df_test_encoded.drop('readmitted', axis=1))\n",
    "print(Y_pred)\n",
    "print(Y_test)\n",
    "print(accuracy_score(Y_test,Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP0169",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
